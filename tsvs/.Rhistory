library(matrixcalc)
library(ggplot2)
library(reshape2)
library(lubridate)
library(xts)
#################
# GLOBAL INPUTS #
#################
# How far back do we download data?
HISTPERIOD <- 11 # Look back 11 years
maxDate <- Sys.Date()-years(HISTPERIOD) # Look back 11 years
# Remove most recent date unless today is the last day of the month
REMOVE.MOST.RECENT.DATE <- T
# Lookback period for optimization input estimation
LOOKBACK <- 11 # How many MONTHS do we look back when performing the optimization?
# Which benchmark do we use?
Benchmark.Ticker <- "BRK-B" # Benchmark Index
# Which assets?
tickers <- c("XLU", "XLV", "XLI", "XLY", "XLK", "XLF", "XLP", "XLE", "XLB")
# How many periods in a year? E.g. 12 for months
periodScale <- 12
################################
# Downloading and merging data #
################################
# Download prices for each stock
StockData <- new.env()
l_ply(tickers, function(ticker) getSymbols(ticker,env = StockData,src = "yahoo",from = maxDate, to = Sys.Date()), .progress = "tk")
prices <- to.monthly(do.call(merge, eapply(StockData, Ad)[tickers]),OHLC=F)
colnames(prices) <- tickers # Rename for charting
Benchmark.prices <- to.monthly(Ad(getSymbols(Benchmark.Ticker, from = maxDate, to = Sys.Date(), src="yahoo", auto.assign = F)), OHLC=F)
# Calculate returns for each stock
returns <- ROC(na.omit(prices), type="discrete")[-1,]
Benchmark.returns <- ROC(na.omit(Benchmark.prices), type="discrete")[-1,]
colnames(Benchmark.returns) <- "Benchmark"
# Combine Returns into One DataFrame including Benchmark
CombinedRets <- na.omit(merge(Benchmark.returns, returns))
paste("Your combined data including the Benchmark goes back to",index(CombinedRets)[1])
if(REMOVE.MOST.RECENT.DATE){
# TBD: Check if today is the last day of the month
paste("We are removing the most recent data point since today is not the last day of the month")
CombinedRets <- head(CombinedRets,-1)
}
paste("Your combined data leads up until",index(tail(CombinedRets,1)))
#############
# LIBRARIES #
#############
library(quantmod)
library(PerformanceAnalytics)
library(plyr)
library(quadprog)
library(Matrix)
library(matrixcalc)
library(ggplot2)
library(reshape2)
library(lubridate)
library(xts)
#################
# GLOBAL INPUTS #
#################
# How far back do we download data?
HISTPERIOD <- 11 # Look back 11 years
maxDate <- Sys.Date()-years(HISTPERIOD) # Look back 11 years
# Remove most recent date unless today is the last day of the month
REMOVE.MOST.RECENT.DATE <- T
# Lookback period for optimization input estimation
LOOKBACK <- 11 # How many MONTHS do we look back when performing the optimization?
# Which benchmark do we use?
Benchmark.Ticker <- "BRK-B" # Benchmark Index
# Which assets?
tickers <- c("XLU", "XLV", "XLI", "XLY", "XLK", "XLF", "XLP", "XLE", "XLB")
# How many periods in a year? E.g. 12 for months
periodScale <- 12
################################
# Downloading and merging data #
################################
# Download prices for each stock
StockData <- new.env()
l_ply(tickers, function(ticker) getSymbols(ticker,env = StockData,src = "yahoo",from = maxDate, to = Sys.Date()))
prices <- to.monthly(do.call(merge, eapply(StockData, Ad)[tickers]),OHLC=F)
colnames(prices) <- tickers # Rename for charting
Benchmark.prices <- to.monthly(Ad(getSymbols(Benchmark.Ticker, from = maxDate, to = Sys.Date(), src="yahoo", auto.assign = F)), OHLC=F)
# Calculate returns for each stock
returns <- ROC(na.omit(prices), type="discrete")[-1,]
Benchmark.returns <- ROC(na.omit(Benchmark.prices), type="discrete")[-1,]
colnames(Benchmark.returns) <- "Benchmark"
# Combine Returns into One DataFrame including Benchmark
CombinedRets <- na.omit(merge(Benchmark.returns, returns))
paste("Your combined data including the Benchmark goes back to",index(CombinedRets)[1])
if(REMOVE.MOST.RECENT.DATE){
# TBD: Check if today is the last day of the month
paste("We are removing the most recent data point since today is not the last day of the month")
CombinedRets <- head(CombinedRets,-1)
}
paste("Your combined data leads up until",index(tail(CombinedRets,1)))
install.packages("tlctk")
install.packages("tcltk")
knitr::opts_chunk$set(echo = TRUE)
### Load packages of interest
#if(!requireNamespace("devtools")) install.packages("devtools")
#devtools::install_github("dkahle/ggmap", ref = "tidyup", force=TRUE)
#uncomment to install packages
#install.packages(c("plyr", "dplyr", "tidyr", "readxl",
#                   "DataExplorer","sp","rgdal","PBSmapping",
#                   "maptools","spdplyr","sf","ggplot2",
#                   "ggpubr","ggmap","GGally","raster","rgeos"))
library(plyr) #data manipulation
library(dplyr) #data manipulation
library(tidyr) #data manipulation
library(readxl) #loading xls files
library(DataExplorer) #EDA
library(sp) #spatial analysis
library(rgdal) #loading shape data
library(PBSmapping)
library(maptools)
library(spdplyr) #assist with dplyr use
library(sf) #popular spatial data package for shapefile
library(ggplot2) #pretty graphics
library(ggpubr)
library(ggmap) #google maps
library(GGally) #parallel coordinate plots
library(raster) #density calculations
#api key for googlemaps
ggmap::register_google(key = "AIzaSyAYFRgyWXXprohnoMyo8v0-tQsbIWhQC9I")
#set dir, regardless of the working directory
#please store "Population Data 191216.xlsx" in the same directory
setwd("~/Downloads/Resume/SGS Application/SGS Graduate Data Analyst Test 191216")
#data importing
input_file <- "Population Data 191216.xlsx"
df_pop<- read_excel(input_file) #dataframe of data
options(warn=-1)
#required function for linear interpolation
new.pop <- function(currentyear=2018,zone=1:nrow(df_pop)) {
inityear <-round_any(currentyear,5,f = floor)+1
futureyear <-round_any(currentyear,5,f = ceiling)+1
iyear<-paste("POP_",inityear,sep="")
fyear<-paste("POP_",futureyear,sep="")
pop<-(df_pop[[fyear]][zone]-df_pop[[iyear]][zone])/
(futureyear-inityear)*(currentyear-inityear)+df_pop[[iyear]][zone]
df_new.pop<-df_pop
df_new.pop$pred_2018<-pop
return(df_new.pop)
}
#required function for linear interpolation
new.pop <- function(currentyear=2018,zone=1:nrow(df_pop)) {
inityear <-round_any(currentyear,5,f = floor)+1
futureyear <-round_any(currentyear,5,f = ceiling)+1
iyear<-paste("POP_",inityear,sep="")
fyear<-paste("POP_",futureyear,sep="")
pop<-(df_pop[[fyear]][zone]-df_pop[[iyear]][zone])/
(futureyear-inityear)*(currentyear-inityear)+df_pop[[iyear]][zone]
df_new.pop<-df_pop
df_new.pop$pred_2018<-pop
return(df_new.pop)
}
head(df_new.pop)
head(new.pop())
#required function for linear interpolation
new.pop <- function(currentyear=2018,zone=1:nrow(df_pop)) {
inityear <-round_any(currentyear,5,f = floor)+1
futureyear <-round_any(currentyear,5,f = ceiling)+1
iyear<-paste("POP_",inityear,sep="")
fyear<-paste("POP_",futureyear,sep="")
pop<-(df_pop[[fyear]][zone]-df_pop[[iyear]][zone])/
(futureyear-inityear)*(currentyear-inityear)+df_pop[[iyear]][zone]
df_new.pop<-df_pop
df_new.pop$pred<-pop
return(df_new.pop)
}
head(new.pop())
df_new_pop <- df_pop
df_new_pop[df_new_pop<0]<-0
head(df_new_pop)
df_new_lga <- df_new_pop %>%
group_by(LGA) %>%
summarise_at(vars(POP_2026,POP_2031),funs(sum))
df_old_lga <- df_pop %>%
group_by(LGA) %>%
summarise_at(vars(POP_2026,POP_2031),funs(sum))
df_old_lga_16_21<- df_pop %>%
group_by(LGA) %>%
summarise_at(vars(POP_2016,POP_2021),funs(sum))
#data frames with the respective populations totals for each LGA
df_old_lga_pop<- left_join(df_old_lga_16_21,df_old_lga , by = 'LGA')
df_new_lga_pop<- left_join(df_old_lga_16_21,df_new_lga , by = 'LGA')
Head(df_new_lga_pop)
df_new_lga <- df_new_pop %>%
group_by(LGA) %>%
summarise_at(vars(POP_2026,POP_2031),funs(sum))
df_old_lga <- df_pop %>%
group_by(LGA) %>%
summarise_at(vars(POP_2026,POP_2031),funs(sum))
df_old_lga_16_21<- df_pop %>%
group_by(LGA) %>%
summarise_at(vars(POP_2016,POP_2021),funs(sum))
#data frames with the respective populations totals for each LGA
df_old_lga_pop<- left_join(df_old_lga_16_21,df_old_lga , by = 'LGA')
df_new_lga_pop<- left_join(df_old_lga_16_21,df_new_lga , by = 'LGA')
head(df_new_lga_pop)
#data frame with the respective ratios to multiply out for 2026 and 2031
df_ratios <- left_join(df_old_lga, df_new_lga, by = 'LGA') %>%
transmute(LGA,POP_2026 = POP_2026.x/POP_2026.y,POP_2031 = POP_2031.x/POP_2031.y)
#data frame with the new adjusted values for 2026 and 2031
df_with_ratios <- left_join(df_new_pop, df_ratios, by = 'LGA')  %>%
transmute(ZONE,LGA,POP_2026 = POP_2026.x*POP_2026.y,POP_2031 = POP_2031.x*POP_2031.y)
#data frame with the new adjusted total population values for 2026 and 2031
df_adjusted_sum <- df_with_ratios %>%
group_by(LGA) %>%
summarise_at(vars(POP_2026,POP_2031),funs(sum))
head(df_adjusted_sum)
#equality checker for new adjusted total population
#and old total population values for 2026 and 2031
head(df_old_lga)
all.equal(df_adjusted_sum,df_old_lga)
#import downloaded shape file data for the plots
shape <- read_sf(dsn = ".", layer = "LGA11aAust")
#cleaning of the downloaded shape file to match with our respective rows names
victoria_sf<-shape %>% filter(shape$STATE_CODE==2) %>%
dplyr::select(-c(STATE_CODE,LGA_CODE11)) %>%
rename(LGA = LGA_NAME11)
#construction of shapefile data frame with the population attributes for 2016 to 2031
victoria_pop_sf<-left_join(left_join(victoria_sf, df_old_lga_16_21, by = 'LGA'),df_adjusted_sum, by = 'LGA')
#needed for naming the respective LGA
world_points<- st_centroid(victoria_pop_sf)
world_points <- cbind(victoria_pop_sf, st_coordinates(st_centroid(victoria_pop_sf$geometry)))
#collects the map from google using the APIs
map <- get_googlemap(center = c(lon = 145.25, lat = -37.8136),
zoom = 9, scale = 2,
maptype ='terrain',
color = 'color', style = c(feature = "road.highway", element = "labels", visibility = "off"))
#to examine terrain
ggmap(map)+ggtitle("Terrain plot of Victoria")
#for loop defining all plots for 2016 to 2031
population_years <-c("POP_2016","POP_2021","POP_2026","POP_2031")
for (i in population_years) {
assign(paste("plot_", i, sep = ""),
ggmap(map) +
geom_sf(data=victoria_pop_sf, aes_string(fill=i), inherit.aes = FALSE) +
scale_fill_viridis_c(option = "plasma", na.value="transparent", limits=c(5500, 465500), direction=-1, alpha = .6) +
geom_sf_text(data=victoria_pop_sf[c(14,47),],aes(label = LGA), colour = "blue",inherit.aes = FALSE) +
guides(fill=guide_legend(title="Population")))
}
#data check for suspect values
ggplot(data=alt, aes(x=LGA, y=POP_2016)) + geom_bar(stat="identity")
#adjusted data with old data for 2016 and 2021, then adjusted data for 2026 and 2031
df_adjusted_sum_all_dates<-left_join(df_old_lga_16_21,df_adjusted_sum,by="LGA")
#finds the minimum and maximum population values that
#will help later with the gradient color plots
minimum<-min(df_adjusted_sum_all_dates[2:5])
maximum<-max(df_adjusted_sum_all_dates[2:5])
bounds<-c(minimum,maximum)
#import downloaded shape file data for the plots
shape <- read_sf(dsn = ".", layer = "LGA11aAust")
#cleaning of the downloaded shape file to match with our respective rows names
victoria_sf<-shape %>% filter(shape$STATE_CODE==2) %>%
dplyr::select(-c(STATE_CODE,LGA_CODE11)) %>%
rename(LGA = LGA_NAME11)
#construction of shapefile data frame with the population attributes for 2016 to 2031
victoria_pop_sf<-left_join(left_join(victoria_sf, df_old_lga_16_21, by = 'LGA'),df_adjusted_sum, by = 'LGA')
#needed for naming the respective LGA
world_points<- st_centroid(victoria_pop_sf)
world_points <- cbind(victoria_pop_sf, st_coordinates(st_centroid(victoria_pop_sf$geometry)))
#collects the map from google using the APIs
map <- get_googlemap(center = c(lon = 145.25, lat = -37.8136),
zoom = 9, scale = 2,
maptype ='terrain',
color = 'color', style = c(feature = "road.highway", element = "labels", visibility = "off"))
#to examine terrain
ggmap(map)+ggtitle("Terrain plot of Victoria")
#for loop defining all plots for 2016 to 2031
population_years <-c("POP_2016","POP_2021","POP_2026","POP_2031")
for (i in population_years) {
assign(paste("plot_", i, sep = ""),
ggmap(map) +
geom_sf(data=victoria_pop_sf, aes_string(fill=i), inherit.aes = FALSE) +
scale_fill_viridis_c(option = "plasma", na.value="transparent", limits=c(5500, 465500), direction=-1, alpha = .6) +
geom_sf_text(data=victoria_pop_sf[c(14,47),],aes(label = LGA), colour = "blue",inherit.aes = FALSE) +
guides(fill=guide_legend(title="Population")))
}
#data check for suspect values
ggplot(data=df_adjusted_sum_all_dates, aes(x=LGA, y=POP_2016)) + geom_bar(stat="identity")
#data check for suspect values
ggplot(data=df_adjusted_sum_all_dates, aes(x=LGA, y=POP_2016)) + geom_bar(stat="identity")+ggtitle("Bar graph of populations of LGA")
df_adjusted_sum_all_dates[2:5]
df_adjusted_sum_all_dates[c(1,2:5)]
#adjusted data with old data for 2016 and 2021, then adjusted data for 2026 and 2031
df_adjusted_sum_all_dates<-left_join(df_old_lga_16_21,
df_adjusted_sum,by="LGA")
#finds the minimum and maximum population values that
#will help later with the gradient color plots
minimum<-min(df_adjusted_sum_all_dates[2:5])
maximum<-max(df_adjusted_sum_all_dates[2:5])
bounds<-c(minimum,maximum)
#import downloaded shape file data for the plots
shape <- read_sf(dsn = ".", layer = "LGA11aAust")
#cleaning of the downloaded shape file to match with our respective rows names
victoria_sf<-shape %>% filter(shape$STATE_CODE==2) %>%
dplyr::select(-c(STATE_CODE,LGA_CODE11)) %>%
rename(LGA = LGA_NAME11)
#construction of shapefile data frame with the population attributes for 2016 to 2031
victoria_pop_sf<-left_join(left_join(victoria_sf, df_old_lga_16_21, by = 'LGA'),
df_adjusted_sum, by = 'LGA')
#needed for naming the respective LGA
world_points<- st_centroid(victoria_pop_sf)
world_points <- cbind(victoria_pop_sf,
st_coordinates(st_centroid(victoria_pop_sf$geometry)))
#collects the map from google using the APIs
map <- get_googlemap(center = c(lon = 145.25, lat = -37.8136),
zoom = 9, scale = 2,
maptype ='terrain',
color = 'color', style = c(feature = "road.highway",
element = "labels", visibility = "off"))
#to examine terrain
ggmap(map)+ggtitle("Terrain plot of Victoria")
#for loop defining all plots for 2016 to 2031
population_years <-c("POP_2016","POP_2021","POP_2026","POP_2031")
for (i in population_years) {
assign(paste("plot_", i, sep = ""),
ggmap(map) +
geom_sf(data=victoria_pop_sf, aes_string(fill=i), inherit.aes = FALSE) +
scale_fill_viridis_c(option = "plasma", na.value="transparent",
limits=c(5500, 465500), direction=-1, alpha = .6) +
geom_sf_text(data=victoria_pop_sf[c(14,47),],aes(label = LGA),
colour = "blue",inherit.aes = FALSE) +
guides(fill=guide_legend(title="Population")))
}
#data check for suspect values
ggplot(data=df_adjusted_sum_all_dates[1:16], aes(x=LGA, y=POP_2016))+
geom_bar(stat="identity")+ggtitle("Bar graph of populations of LGA")
#data check for suspect values
ggplot(data=df_adjusted_sum_all_dates[1:16,], aes(x=LGA, y=POP_2016))+
geom_bar(stat="identity")+ggtitle("Bar graph of populations of LGA")
ggplot(data=df_adjusted_sum_all_dates[c(1,17:32),], aes(x=LGA, y=POP_2016))+
geom_bar(stat="identity")+ggtitle("Bar graph of populations of LGA")
plot_POP_2016+ggtitle("Population for 2016")
#adjusted data with old data for 2016 and 2021, then adjusted data for 2026 and 2031
df_adjusted_sum_all_dates<-left_join(df_old_lga_16_21,
df_adjusted_sum,by="LGA")
#finds the minimum and maximum population values that
#will help later with the gradient color plots
minimum<-min(df_adjusted_sum_all_dates[2:5])
maximum<-max(df_adjusted_sum_all_dates[2:5])
bounds<-c(minimum,maximum)
#import downloaded shape file data for the plots
shape <- read_sf(dsn = ".", layer = "LGA11aAust")
#cleaning of the downloaded shape file to match with our respective rows names
victoria_sf<-shape %>% filter(shape$STATE_CODE==2) %>%
dplyr::select(-c(STATE_CODE,LGA_CODE11)) %>%
rename(LGA = LGA_NAME11)
#construction of shapefile data frame with the population attributes for 2016 to 2031
victoria_pop_sf<-left_join(left_join(victoria_sf, df_old_lga_16_21, by = 'LGA'),
df_adjusted_sum, by = 'LGA')
#needed for naming the respective LGA
world_points<- st_centroid(victoria_pop_sf)
world_points <- cbind(victoria_pop_sf,
st_coordinates(st_centroid(victoria_pop_sf$geometry)))
#collects the map from google using the APIs
map <- get_googlemap(center = c(lon = 145.25, lat = -37.8136),
zoom = 9, scale = 2,
maptype ='terrain',
color = 'color', style = c(feature = "road.highway",
element = "labels", visibility = "off"))
#for loop defining all plots for 2016 to 2031
population_years <-c("POP_2016","POP_2021","POP_2026","POP_2031")
for (i in population_years) {
assign(paste("plot_", i, sep = ""),
ggmap(map) +
geom_sf(data=victoria_pop_sf, aes_string(fill=i), inherit.aes = FALSE) +
scale_fill_viridis_c(option = "plasma", na.value="transparent",
limits=c(5500, 465500), direction=-1, alpha = .6) +
geom_sf_text(data=victoria_pop_sf[c(14,47),],aes(label = LGA),
colour = "blue",inherit.aes = FALSE) +
guides(fill=guide_legend(title="Population")))
}
plot_POP_2016+ggtitle("Population for 2016")
areashp<-readOGR(".","LGA11aAust")
areashp$Totalarea<-area(areashp)
victoria_sf_area<-areashp %>% filter(shape$STATE_CODE==2) %>%
dplyr::select(-c(STATE_CODE,LGA_CODE11)) %>%
rename(LGA = LGA_NAME11)
#introduces an area column to assist in population density calculation
victoria_pop_sf$totalarea<-victoria_sf_area$Totalarea
#density data frame
df_with_pop_density <- victoria_pop_sf  %>%
transmute(LGA,geometry,POP_2016,POP_2021,POP_2026,POP_2031,
pop_density_2016 = POP_2016/totalarea,
pop_density_2021 = POP_2021/totalarea,
pop_density_2026 = POP_2026/totalarea,
pop_density_2031 = POP_2031/totalarea)
population_years <-c("pop_density_2016","pop_density_2021","pop_density_2026","pop_density_2031")
for (i in population_years) {
assign(paste("plot_", i, sep = ""),
ggmap(map) +
geom_sf(data=df_with_pop_density, aes_string(fill=i), inherit.aes = FALSE) +
scale_fill_viridis_c(option = "plasma", na.value="transparent", limits=c(0, 0.0075), direction=-1, alpha = .6) +
geom_sf_text(data=victoria_pop_sf[c(14,47),],aes(label = LGA), colour = "blue",inherit.aes = FALSE) +
guides(fill=guide_legend(title="Density")))
}
plot_pop_density_2016+ggtitle("Population Density for 2016")
victoria_pop_sf[LGA=="Casey (C)"]
victoria_pop_sf
victoria_pop_sf[LGA=="Casey (C)",]
victoria_pop_sf[victoria_pop_sf$LGA=="Casey (C)",]
#an array of all plots
ggarrange(plot_POP_2016+ggtitle("Population for 2016")+theme(plot.margin = unit(c(0, 0, 0, 0), "cm")),
plot_POP_2021+ggtitle("Population for 2021")+theme(plot.margin = unit(c(0, 0, 0, 0), "cm")),
plot_POP_2026+ggtitle("Population for 2026")+theme(plot.margin = unit(c(0, 0, 0, 0), "cm")),
plot_POP_2031+ggtitle("Population for 2031")+theme(plot.margin = unit(c(0, 0, 0, 0), "cm")),
ncol = 2, nrow = 2)
ggparcoord(data=df_adjusted_sum_all_dates[c(1,17:32),], columns = 2:5, groupColumn = "LGA", scale = 'globalminmax', showPoints = TRUE)
ggarrange(plot_POP_2016+ggtitle("Population for 2016")+theme(plot.margin = unit(c(-1, 0, 0, 0), "cm")),
plot_POP_2021+ggtitle("Population for 2021")+theme(plot.margin = unit(c(0, 0, 0, 0), "cm")),
plot_POP_2026+ggtitle("Population for 2026")+theme(plot.margin = unit(c(0, 0, 0, 0), "cm")),
plot_POP_2031+ggtitle("Population for 2031")+theme(plot.margin = unit(c(0, 0, 0, 0), "cm")),
ncol = 2, nrow = 2)
install.packages("ohenery")
install.packages('devtools')
install.packages('devtools')
devtools::install_github("cancer-evolution/SPIAT")
devtools::install_github("JinmiaoChenLab/Rphenograph")
devtools::install_github("cancer-evolution/SPIAT")
q()
setwd("~/Desktop/Microbial Project/Reading 16s BIOM file 2")
load("~/Desktop/Microbial Project/Reading 16s BIOM file 2/Micro.RData")
#organise by family
library(phyloseq); packageVersion("phyloseq")
load("~/Desktop/Microbial Project/Reading 16s BIOM file 2/Micro.RData")
library(biomformat)
library(dplyr)
library(dada2)
library(matR)
setwd("~/Desktop/Microbial Project/Reading 16s BIOM file 2/Data/all")
# biom_file <- system.file("Data/all", "all.biom", package = "biomformat")
# biom_file
raw_data<-biom_data(read_biom("all.biom"))
# x_merged <-merge(x1_new, x2_new, by = "row.names", all = TRUE)
# x_merged[is.na(x_merged)] <- 0
#fix files
for (i in list.files()) {
assign(paste0('counts_', gsub(" ", "_", i)), as(biom_data(read_biom(i)),"matrix"))
}
counts_bio <- list(
counts_all.biom_3,
counts_all.biom_4,
counts_all.biom_5,
counts_all.biom_6,
counts_all.biom_7
)
x_merged <- merge(counts_all.biom, counts_all.biom_2,
by = "row.names",
all = TRUE)
for (counts_tbl in counts_bio) {
#fixes the index
result <- x_merged[-1]
row.names(result) <- x_merged$Row.names
#merging
x_merged <- merge(result, counts_tbl,
by = "row.names",
all = TRUE)
}
#makes NAs into zeroes
x_merged[is.na(x_merged)] <- 0
setwd("~/Desktop/Microbial Project/Reading 16s BIOM file 2/Data/tsvs")
library(dplyr)
library(readr)
df_tsv <- list.files() %>%
lapply(function(i) {
read.csv(i,sep = "\t")})
#columns to keep
keeps <- colnames(df_tsv[[7]])
df_tsv2 <- df_tsv %>%
lapply(function(i) {select(i,keeps)})
#fixing the annoying data set.
df_tsv2[[6]]$level_of_colonisation <- as.character(df_tsv2[[6]]$level_of_colonisation)
df_tsv2[[6]]$group_level_of_colonisation <- as.character(df_tsv2[[6]]$group_level_of_colonisation)
#finally binding rows
df_tsv3 <- df_tsv2 %>% bind_rows()
#code modification
mouse_id<-c("Mouse_E1","Mouse_E2","Mouse_E3","Mouse_E4","Mouse_E5")
#META DATA
df_vanco <- df_tsv3 %>%
filter(antibiotic=="Vancomycin", host_subject_id %in% mouse_id) %>%
group_by(host_subject_id) %>%
arrange(day_of_experiment_numerical, .by_group = TRUE)
View(df_vanco)
#filter out low counts with only
df_present <- df_vanco %>% select(sample_id, title, antibiotic,
phase_of_experiment,
level_of_colonisation,
route_abx_administration,
day_of_experiment_numerical,
average_cfu_counts)
View(df_vanco)
View(df_presenet)
View(df_present)
df_present %>% filter(host_subject_id='Mouse_E1')
df_present %>% filter(host_subject_id=='Mouse_E1')
df_present %>% filter(host_subject_id=='Mouse_E1') %>% select(average_cfu_counts)
hehe <- df_present %>% filter(host_subject_id=='Mouse_E1') %>% select(average_cfu_counts)
plot(hehe)
hehe <- df_present %>% filter(host_subject_id=='Mouse_E1') %>% select(average_cfu_counts %>% ungroup())
hehe <- df_present %>% filter(host_subject_id=='Mouse_E1') %>% select(average_cfu_counts)
hehe <- df_present %>% filter(host_subject_id=='Mouse_E1') %>% ungroup() %>% select(average_cfu_counts)
hehe
plot(hehe)
plot(1:37,hehe)
ggplot(hehe, aes(x = 1:37, y = average_cfu_counts)) + geom_point()
library(ggplot2)
ggplot(hehe, aes(x = 1:37, y = average_cfu_counts)) + geom_point()
View(phyloseq_Mouse_E1)
View(otu_table(phyloseq_Mouse_E1))
otu_table(phyloseq_Mouse_E1)
otu_table(phyloseq_Mouse_E2)
